{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Near Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a set of data points and a query $Q$ and we have to answer what is the nearest data point to a query. \n",
    "\n",
    "The data points are known to us, so we can preprocess the data beforehand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "There are numerious applications to fining nearest neighbors, some of which are as follows:\n",
    "\n",
    "1. Finding Similar Images in Search\n",
    "\n",
    "2. Finding Near Duplicate Webpages/Articles - Specifically used in de-duplicating the websites. For eg:- While implemnting a search engine, we don't want multiple copies of the same article with only minor variations, so we perform de-duplication.\n",
    "\n",
    "3. Clustering - Given a particular datapoint, we want to find the items that are close to it. \n",
    "\n",
    "4. Nearest Neighbor Classifier - In a nearest neighbor classifier, given a test point, we look at the labels of the neighbors around it and assign a label to the test point based on the labels of the neighbors. So naturally, before we assign a label, we need to find the neighbors efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants \n",
    "\n",
    "Anoter variant of this algorithm is All Pairs Near Neighbor. In the variation, rather than for a particular query, given a dataset we need to find all pairs of datapoints that are considered near neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Scan\n",
    "\n",
    "Given a query, we go over all the data points. Assuming that the points are vectors in $\\mathbb{R}^d$, then we $O(nd)$ time for each query, where $n$ is the no. of points, because we compare each point to the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voronoi Partition\n",
    "\n",
    "Given a set a point, we compute the Voronoi Partition of the points. Suppose we are inn $\\mathbb{R}^2$, the following image shows the voronoi partition. Then, given a point we mark off all the regions that are closest to that particular point, which will be the Voronoi cell. The following image shows the voronoi cell for one point. \n",
    "\n",
    "```{figure} ../assets/2022_04_07_Near_Neighbors/voronoi_cell.jpg\n",
    "\n",
    ":name: Voronoi Paritioning\n",
    "\n",
    "Voronoi Parition with regions marked closest to the pink point[\\[Source\\]](http://pcg.wikidot.com/pcg-algorithm:voronoi-diagram)\n",
    "```\n",
    "\n",
    "We do this for all the points. Then, given a query, we just need to check which Voroni Partition it falls in. \n",
    "\n",
    "Unfortunately, Voronoi Partition in $d$ dimensions needs a lot of storage. For $n$ points in $d$ dimensions, it needs $n^\\frac{d}{2}$ storage, which is too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Partitioning Trees\n",
    "\n",
    "Given some datapoints, we recursively partition the space. In the following image, we first paritioned the space in x-axis and y-axis. The we further partition each quarter with their relative x-axis and y-axis. \n",
    "\n",
    "```{image} ../assets/2022_04_07_Near_Neighbors/spt-axis.jpeg\n",
    ":alt: PartitionedSpace\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "In the tree, the top level partition will have 4 children corresponding the 4 axis. And each of those partitions will have 4 children, corresponding to the 4 axis and so on. \n",
    "\n",
    "```{image} ../assets/2022_04_07_Near_Neighbors/spt-tree.jpeg\n",
    ":alt: PartitionedTree\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Given a query point, we node we ask which children we need to traverse. The partitioning method mentioned above is very naive. Deciding how to the partitioning is the most critical part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kd-Trees\n",
    "\n",
    "Originally, $k$ used to denote the number of dimensions, for e.g. $2d$ or $3d$ trees. Our idea here is that, each level of the tree each uses a single dimension to partition. \n",
    "\n",
    "1. Starting with the entire set of points, with each level of tree, we associate a cutting dimension. \n",
    "\n",
    "2. We then cycle through these dimensions. For e.g., if we start with the first dimension and we form the first cut according to some threshold in the first dimension. Then at the second level we use the second dimension and choose some threshold level in second dimension and so on.\n",
    "\n",
    "3. For choosing the threshold, at every step, we try to balance the tree. To do so, we choose the point which is the median along that dimension and create an axis-aligned partition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here is an exmaple of kd tree for 2 dimentions.\n",
    "\n",
    "* First, we devide a space with a line passing thorugh a median point of datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../assets/2022_04_07_Near_Neighbors/example1.jpeg\n",
    ":alt: example1\n",
    ":width: 400px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, we devide S1 space into two by its median point, and same with S2 space. \n",
    "\n",
    "* We devide every subspace untill all points are covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ../assets/2022_04_07_Near_Neighbors/example2.jpeg\n",
    ":alt: example2\n",
    ":width: 400px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With every node, there is a data point associated with that as we are using median point for division. The leaves node also contain datapoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "* **Space taken** = $O (n)$\n",
    "\n",
    "* **Nearest neighbour Search**\n",
    "\n",
    "    - **Defeatist Search:** \n",
    "        - Only search the child that contain the query point. \n",
    "        - Does not give always a correct answer, as a nearest neighbour point might be in another cell.\n",
    "\n",
    "    - **Desending Search:**\n",
    "        - Maintain the current near neighbour and distance to it. \n",
    "        - Visit one or both children depending on wheather there is intersection.\n",
    "        - If there is an intersection check that cell also.\n",
    "        - Defenitely gives a nearest neighbour but might end up searching antire tree.\n",
    "        \n",
    "    - **Priority Search:**\n",
    "        - Maintain a priority queue of the regions depending on distance.\n",
    "        - Decide to visit the cell by the priority of the distance to the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer>\n",
    "    Author(s): Divyanshu Meena, Harshil Purohit, Mihir Chauhan\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
