{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Numerical Linear Algebra: Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Introduction</b></h3>\n",
    "\n",
    "Random projection is a method to find the lower rank approximation of matrices in faster time and low error rate. We will see how we can find such a lower rank matrix and its applications in this chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Johnson Lindenstrauss Lemma</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:lemma} Johnson Lindenstrauss Lemma [JL84]\n",
    ":label: JL_Lemma\n",
    ":nonumber:\n",
    "\n",
    "Given a set of points $x_1, x_2, ...., x_n \\in \\mathcal{R}^d$, there exists a linear mapping $A$ that creates the images of these points in the target dimension $k$ such that the pairwise distance between any two points is preserved by the factor $\\epsilon$, where $k \\geq \\frac{c\\log{n}}{\\epsilon^2}, \\epsilon > 0$. i.e. for all $(i, j)$\n",
    "\n",
    "```{math}\n",
    ":label: JL_EQ\n",
    "\n",
    "(1 - \\epsilon)\\|x_i - x_j\\|_2 \\leq \\|Ax_i - Ax_j\\|_2 \\leq (1 + \\epsilon)\\|x_i - x_j\\|_2\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "$Ax_1, Ax_2, ...., Ax_n \\in \\mathcal{R}^k$\n",
    "```\n",
    "\n",
    "```{figure} ../assets/2022_04_07_Randomized_Numerical_Linear_Algebra_Random_Projection/projection.jpg\n",
    ":name: projection\n",
    ":align: center\n",
    ":width: 500px\n",
    "Projection to Lower Dimension\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping is created by filling up the matrix with independant gaussian random variables. The guarantee is that with very high probability for every pair $(i, j)$ the distance $\\|Ax_i - Ax_j\\|_2$ will be within $(1 \\pm \\epsilon)\\|x_i - x_j\\|_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Other Properties</b></h3>\n",
    "\n",
    "Remember that a given length $x$ is preserved with probability $1 - \\delta$ if the target dimension $k$ is $\\frac{c}{\\epsilon^2} \\log(\\frac{1}{\\delta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:property}\n",
    ":label: JL_PROP_1\n",
    "\n",
    "It is known that the bound $k \\geq \\frac{c}{\\epsilon^2} \\log(\\frac{1}{\\delta})$ is tight.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:property}\n",
    ":label: JL_PROP_2\n",
    "\n",
    "Target dimension depends only on $\\epsilon$  and $\\delta$. It does not depend on the original dimension.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:property}\n",
    ":label: JL_PROP_3\n",
    "\n",
    "Such a result cannot hold on distance metrics other that Euclidean metric. For example, it will not hold for $L_1$ Norm.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Other Constructions</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Earlier Method:</h4>\n",
    "\n",
    "We were creating the matrix $A$ by creating the matrix $R$ such that $R_{ij} \\sim N(0, 1)$. We make sure that every column of $R$ is normalized. So, \n",
    "\n",
    "\\begin{align}\n",
    "A = \\frac{1}{\\sqrt{k}}R\n",
    "\\end{align}\n",
    "\n",
    "We divide by $\\sqrt{k}$ so that the expected norm of each column becomes $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is expensive to sample, create and store Gaussian random variables. We might also need to store floating point numbers. So, instead of using Gaussian random variables the matrix $R$ is created where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: my-definition-1\n",
    ":nonumber:\n",
    "\n",
    "$$ R_{ij} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    +1, & p = \\frac{1}{3} \\\\\n",
    "    0, & p = \\frac{2}{3} \\\\\n",
    "    -1, & p = \\frac{1}{3} \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "where, $p$ = probability.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Example Application: PCA</b></h3>\n",
    "\n",
    "Suppose we have $A \\in \\mathcal{R}^{n \\times d}$. We want a rank-$k$ approximation $A'$ such that $\\|A - A'\\|_F$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here, $\\|A - A'\\| = \\sum_{i, j}(A - A')_{ij}^2$ is the frobenius norm.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The optimal low rank approximation is $A_k = U_k \\Sigma_k V_k^t$, but it takes $\\mathcal{O}(nd \\times min(n,d))$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If $n \\approx d$, then it takes $\\mathcal{O}(n^3)$ time.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Low Rank Approximation</b></h3>\n",
    "\n",
    "We know that $A_k = P_A^kA$, where projection $P_A^k = U_kU_k^t$.\n",
    "\n",
    "Also, \n",
    "\\begin{align}\n",
    "\\|A - P_A^kA\\|_2 = \\sigma_{k+1}\n",
    "\\end{align}\n",
    "For any $B$ and $P_B^k$,\n",
    "\\begin{align}\n",
    "\\sigma_{k+1} \\leq \\|A - P_B^kA\\| \\leq \\sigma_{k+1} + \\sqrt{2\\|AA^t - BB^t\\|}\n",
    "\\end{align}\n",
    "\n",
    "Now, we want $B$ such that,\n",
    "- It is efficiently computable and small\n",
    "- It leads to low error, i.e. $\\|AA^t - BB^t\\| \\leq \\epsilon\\|AA^t\\|_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Cheap and Effective Low Rank</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $A \\in \\mathcal{R}^{n \\times d}$, create $R \\in \\mathcal{R}^{d \\times k}$ as JL matrix.\n",
    "\n",
    "Take $B = AR \\in \\mathcal{R}^{n \\times k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This takes $\\mathcal{O}(ndk)$ time.\n",
    "\n",
    "Also, $E[BB^t] = A E[RR^t] A^t = AA^t$, since every $R_{ij}$ has unit variance and are independent of each other. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>JL in Low Rank Approximation</b></h3>\n",
    "\n",
    "Observe that,\n",
    "\n",
    "\\begin{align}\n",
    "\\|AA^t - BB^t\\| &= \\sup_{\\|x\\| = 1} \\|\\|xA\\|_2^2 - \\|xAR\\|_2^2\\| \\\\\n",
    "&= \\sup_{\\|x\\| = 1}\\|\\|y\\|_2^2 - \\|yR\\|_2^2\\| \\hspace{2cm} \\text{(Take } xA = y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Remember that,\n",
    "\n",
    "\\begin{align}\n",
    "\\|A\\|_2 &= \\sup_{\\|x\\|_2 = 1}\\|Ax\\| \\\\\n",
    "\\|A\\|_2^2 &= \\max_{\\|x\\|_2 = 1} x^TAx\n",
    "\\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the JL property,\n",
    "\\begin{align}\n",
    "Pr[\\|yR\\|^2 - \\|y\\|^2 > \\epsilon\\|y\\|^2] < e^{-ck\\epsilon^2} \\hspace{2cm} \\text{(Exponentially small)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the union bound over a $\\epsilon$ - net, $k = \\tilde{O}(\\frac{rank(A)}{\\epsilon^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{note}\n",
    "```{figure} ../assets/2022_04_07_Randomized_Numerical_Linear_Algebra_Random_Projection/sphere.jpg\n",
    ":name: sphere\n",
    ":align: center\n",
    ":width: 400px\n",
    "$\\epsilon$-net Sphere\n",
    "```\n",
    "$\\epsilon$ - net is when we choose points in unit sphere such that the union of the $\\epsilon$ - radius spheres of all the points give the unit sphere.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get,\n",
    "\\begin{align}\n",
    "\\sup_{\\|x\\| = 1}\\|\\|xA\\|^2 - \\|xAR\\|^2\\| \\leq \\epsilon\\|AA^t\\|\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Time taken for Projection</b></h3>\n",
    "\n",
    "The matrix vector multiplication takes $\\mathcal{O}(kd)$ time, where $k = \\Omega(\\frac{1}{\\epsilon^2})$.\n",
    "\n",
    "Now, there is also a lower bound on the target dimension, so can we make the projection faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Thought Experiment</b></h3>\n",
    "\n",
    "````{prf:definition}\n",
    ":label: my-definition-3\n",
    ":nonumber:\n",
    "Suppose the projection matrix $A$ is very sparse:\n",
    "\n",
    "$$ A_{ij} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    0, & probability = 1 - p \\\\\n",
    "    N(0, \\frac{1}{\\sqrt{p}}), & probability = p \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "Now, set $p$ ~ $\\frac{1}{d}$, then there will be a constant number of entries in most of the rows. So, time taken will be $\\mathcal{O}(dkp)$ ~ $\\mathcal{O}(k)$ only.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{caution}\n",
    "- It fails to preserve norm for sparse vectors.\n",
    "```{figure} ../assets/2022_04_07_Randomized_Numerical_Linear_Algebra_Random_Projection/example.jpg\n",
    ":name: example\n",
    ":align: center\n",
    ":width: 400px\n",
    "Example for which norm is not preserved\n",
    "```\n",
    "- It works fine if vector is all dense!\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to work with all vectors. So, we try to preprocess vectors to make them dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The time taken for pre-processing should not be more than for projection.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Hadamard Matrices</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:definition}\n",
    ":label: my-definition-2\n",
    ":nonumber:\n",
    "Hadamard matrices are defined only when $d = 2^k$. i.e.\n",
    "\n",
    "$H_1 = \\begin{pmatrix}\n",
    "1\n",
    "\\end{pmatrix}$, $H_2 = \\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{pmatrix}$, $H_{2^{k+1}} = \\begin{pmatrix}\n",
    "H_{2^k} & H_{2^k} \\\\\n",
    "H_{2^k} & -H_{2^k}\n",
    "\\end{pmatrix}$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Multiplying a vector by $Hd$ takes $\\mathcal{O}(d\\log{d})$ time.\n",
    "\n",
    "It can be proved by recursion: $T(d) = 2T(\\frac{d}{2}) + \\mathcal{O}(d)$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Densifying using Hadamard</b></h3>\n",
    "\n",
    "Now, we can use the Hadamard matrix to densify $x \\in \\mathcal{R}^d$ as follows:\n",
    "\n",
    "$y = HDx$, where $D \\in \\mathcal{R}^{d \\times d}$ diagonal with,\n",
    "\n",
    "$$ D_{ii} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    +1, & p = \\frac{1}{2} \\\\\n",
    "    -1, & p = \\frac{1}{2} \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "Here, calculating $y$ takes $\\mathcal{O}(d\\log{d})$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Intuition</b></h3>\n",
    "\n",
    "$H$ itself is a rotation,\n",
    "- Sparse vectors are rotated to dense vectors (Uncertainty principle).\n",
    "- But, it is a rotation, it can happen that few dense vectors can become sparse.\n",
    "- Randomization using the diagonal ensures that adversary cannot choose such vectors as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Densification Claim</b></h3>\n",
    "\n",
    "For $x \\in \\mathcal{R}^d,\\|x\\|_2 = 1$,\n",
    "\n",
    "$$max_i\\|(HDx)_i\\| \\leq \\mathcal{O}(\\frac{\\log{(nd)}}{d})^{\\frac{1}{2}}$$\n",
    "\n",
    "The maximum is near the average ($\\frac{1}{\\sqrt{d}}$), so the vector $HDx$ is dense.\n",
    "\n",
    "```{prf:remark}\n",
    ":label: my-remark-1\n",
    ":nonumber:\n",
    "The claim can be proved by applying Cherfnoff style tail inequality per coordinate and union bound.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Projecting a Dense Vector</b></h3>\n",
    "\n",
    "Take, $y = HDx$\n",
    "\n",
    "$\\max_i \\|y_i\\| \\approx \\mathcal{O}(\\sqrt{\\frac{\\log{nd}}{d}})$\n",
    "\n",
    "$$ P = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    0, & probability = 1 - q \\\\\n",
    "    N(0, \\frac{1}{\\sqrt{q}}), & probability = q \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "where, $P \\in \\mathcal{R}^{k \\times d}$ and $k = \\mathcal{O}(\\frac{1}{\\epsilon^2} \\log{\\frac{1}{\\delta}})$\n",
    "\n",
    "And, $z = PHDx$ is the final projected vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Fast JL Transform</b></h3>\n",
    "\n",
    "If $q = \\mathcal{O}(\\|x\\|_\\infty^2) = \\mathcal{O}(\\frac{\\log{(nd)}}{d})$, $PHD$ satisfies $JL$ property.\n",
    "\n",
    "```{prf:observation}\n",
    ":label: my-observation-1\n",
    ":nonumber:\n",
    "Calculating $y = PHDx$ takes time $\\mathcal{O}(d\\log{d} + k\\log{(nd)})$, potentially much faster than original Gaussian construction.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer>\n",
    "    Author(s): Gaurav Viramgami, Hrushti Naik, Manas Mulpuri\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
