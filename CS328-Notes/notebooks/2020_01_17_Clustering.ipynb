{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering  \n",
    "\n",
    "\n",
    "Clustering is one of the forms of unsupervised learning when the data does not have labels.\n",
    "\n",
    "It is useful for:\n",
    "\n",
    "* Detecting patterns  \n",
    "   Example - In image data, customer shopping results, anomalies, etc.    \n",
    "\n",
    "* Optimizing   \n",
    "   Example - Distributing data across various machines, cleaning up search results, facility allocation for city planning, etc.\n",
    "\n",
    "* When we ‚Äòdon‚Äôt know‚Äô what exactly we are looking for\n",
    "\n",
    "## Basic idea\n",
    "\n",
    "Clustering is basically concerned with grouping the objects into a small number of meaningful groups called clusters.   \n",
    "\n",
    "However-   \n",
    "* How do we define the similarity/distance between the objects?\n",
    "* When do we call the groups meaningful?\n",
    "* How many groups should the objects be divided into?\n",
    "\n",
    "So typically, there is no supervision for clustering the objects. Since there is no ground truth, evaluating the quality of clustering is often difficult.   \n",
    "For example, consider the two clustering cases depicted in the image below:\n",
    "\n",
    "```{figure} ../assets/2020_01_17_Clustering/fig1.png\n",
    ":name: fig1\n",
    "\n",
    "\n",
    "```  \n",
    "     \n",
    "What is the right way of clustering - the left or the right? Essentially, the answer depends on the application in use. However, most times, clustering might not have an end goal and there might not be a fixed application to proceed with. In such a case, to define what the right way of clustering is, we need an objective function!    \n",
    "\n",
    "\n",
    "```{caution} There is no unique way of defining clusters that work for all applications.\n",
    "```  \n",
    "---      \n",
    "\n",
    "## Objective Function\n",
    "\n",
    "The most structured way to cluster the objects is the following:   \n",
    "* Specifying the number of clusters required   \n",
    "K centers / K means / K-median\n",
    "\n",
    "* Specifying cluster separation or quality   \n",
    "Dunn‚Äôs index - works on the notion of density, i.e., a set of points would be called a cluster if the points are dense enough  \n",
    "Radius - works on the notion of radius of the cluster, i.e., a set of points would count as a cluster if all the points fall within certain radius from a specific center\n",
    "\n",
    "* Graph-based measures  \n",
    "This extends the objects of clusters from being points to networks.\n",
    "For example, what would the shortest distance mean with respect to networks? Is it the shortest path between the nodes?\n",
    "\n",
    "* Working without an objective function  \n",
    "Hierarchical clustering schemes that define clusters based on some intuitive algorithms\n",
    " \n",
    "---\n",
    "\n",
    "## K-Center\n",
    "\n",
    "### Introduction \n",
    "Consider a cluster with specific cluster centers.  \n",
    "Cost of a point = Distance of the point from the closest cluster center  \n",
    "Cost of a single cluster = Maximum of all the point costs of that cluster  \n",
    "Cost of a complete clustering = Maximum of all the cluster costs, say D    \n",
    "\n",
    "\n",
    "<b> Example:  \n",
    "\n",
    "```{figure} ../assets/2020_01_17_Clustering/fig2.jpeg\n",
    ":name: fig2\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Cost of points: </b>  \n",
    "Point 1 = d11    \n",
    "Point 2 = d12  \n",
    "Point 3 = d23  \n",
    "Point 4 = d24  \n",
    "<b>  \n",
    "Cost of clusters: </b>   \n",
    "Cluster 1 = max(d11, d12) = d12  \n",
    "Cluster 2 = max(d23, d24) = d23  \n",
    "<b>  \n",
    "Cost of clustering:  </b>  \n",
    "D = max(d12, d23) = d23\n",
    "\n",
    "<b> Algorithm‚Äôs Goal: </b> The algorithm has to find the centers C1, C2 such that the total cost of the clustering(D) is minimized, i.e., the distance of the farthest point from the center is minimized.   \n",
    "\n",
    "\n",
    "Consider a set of points as shown in the image. The aim is to choose the positions of k number of centers and create circles corresponding to each center such that all the points in the set lie inside the k circles.\n",
    "\n",
    "\n",
    "```{figure} ../assets/2020_01_17_Clustering/fig3.jpeg\n",
    ":name: fig3\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<b>K = 4  \n",
    "Solution #1: </b>  \n",
    "All four centers are the points of the input set. The radii correspond to the distance of the farthest points from the centers.  \n",
    "Notice that the cost of the cluster(D) = r\n",
    "\n",
    "```{figure} ../assets/2020_01_17_Clustering/fig5.jpeg\n",
    ":name: fig5\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "<b> Solution #2: </b>  \n",
    "All the centers of the circles need not be the points from the input set, but can be arbitrary. Also, although k=4, only three circles have been used to enclose all the points.   \n",
    "However, notice that the cost of the new cluster(D‚Äô) = r‚Äô > r\n",
    "\n",
    "```{figure} ../assets/2020_01_17_Clustering/fig4.jpeg\n",
    ":name: fig4\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Observations\n",
    "\n",
    "* The input set of points should be enclosed in utmost k circles  \n",
    "   * The algorithm gets to choose the centers of the circles\n",
    "   * All the points should be covered within the circles\n",
    "   * Solution value(cost) is given by the largest radii among all the circles  \n",
    "\n",
    "* If we are allowed to k circles, is it possible to obtain a better solution by using k-1 circles instead of k circles? No, because we can always split the biggest ball into two to reduce the cost. It means, given the k centers, we will always use all of them for the optimal solution.\n",
    "\n",
    "* This simple problem of finding optimal K-balls is NP-hard.\n",
    "* However, there is a solvable algorithm. It is an easy 2-approximation, i.e., if the most optimal solution using at most k circles needs a maximum radius OPT, then our algorithm which also uses k circles will have a maximum radius of not more than 2 x OPT.\n",
    "\n",
    "\n",
    "### 2-Approximation Algorithm\n",
    "Let the input set contain a total of 'n' points.  \n",
    "Let all the 'k' centers be the points of the input set.\n",
    "\n",
    "\n",
    "Following is the pseudocode for 2-approximation:   \n",
    "\n",
    "* Choose centers from given points\n",
    "* Choose the first center arbitrarily\n",
    "* For each remaining center:   \n",
    "    * Choose the point farthest from the existing centers as the new center   \n",
    "* Repeat the process until all k centers are chosen\n",
    "\n",
    "\n",
    "<b> Runtime: </b> O(kn)  \n",
    "\n",
    "\n",
    "\n",
    "### Why 2-approximation?\n",
    "\n",
    "Suppose 'D' is th largest radius among K-Centers.   \n",
    "Let,    \n",
    "&emsp; &emsp; g(1), g(2),  . . . . . . g(k) = k centers  \n",
    "\n",
    "&emsp; &emsp; g(k+1) = farthest point from {g(1), g(2), .....g(k)}   \n",
    "\n",
    "&emsp; &emsp; G(i) = {g(1), g(2), . . . g(i)}    \n",
    "\n",
    "&emsp; &emsp;  G(k) = final set    \n",
    " \n",
    " Solution cost = d (g(k+1) , G(k))    \n",
    "\n",
    " $ \\Delta (i) $ =  max  of  d(x, G(i)) &ensp; i.e farthest from G(i)} \n",
    "\n",
    "```{prf:lemma}\n",
    ":label: my-lemma\n",
    "\n",
    "$ \\Delta (i+1) \\le  \\Delta   (i)$\n",
    "```\n",
    "\n",
    "```{prf:proof}    \n",
    "\n",
    "\n",
    "$ \\Delta (i+1) =$ max of d (x, G(i)) =  d (g(i+1), G(i))   \n",
    "\n",
    "Also,  d(g(i+1), G(i))   $\\le$   d(g(i+1), G(i-1)) ... Since our algorithm is greedy   \n",
    "\n",
    "Now,  d(g(i+1), G(i-1))   $\\le$    d(g(i), G(i-1))  =  $\\Delta (i) $  \n",
    "\n",
    "Hence    $ \\Delta (i+1)  \\le  \\Delta   (i)$\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: my-lemma2\n",
    "\n",
    "For any two  i, j  where  j < i,    \n",
    "\n",
    "$$  d(g(i), g(j))  \\ge  \\Delta  (k)  $$\n",
    "```\n",
    "\n",
    "```{prf:proof}    \n",
    "\n",
    "\n",
    "d(g(i), g(j)) $\\ge$  d(g(i), G(i-1))  =  $\\Delta   (i)$    \n",
    "\n",
    "Also,  $ \\Delta   (i)  \\ge  \\Delta  (k) $  ... from previous lemma   \n",
    "\n",
    "Putting these together for any i, j   \n",
    "\n",
    "d(g(i), g(j))  $\\ge   \\Delta  _{min} (i, j) \\ge   \\Delta   (k)$   \n",
    "\n",
    "d(g(k+1), g(i))  $ \\ge  $  d(g(k+1), G(k))  =  $ \\Delta  (k)$  \n",
    "\n",
    "Hence, when we consider G(k+1), all pairs of points in G(k+1) are separated by atleast $\\Delta  (k)$\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: my-lemma3\n",
    "\n",
    "Suppose O be the K-Centers of the optimal solution and the associated cost be $ \\Delta (O)$  ,  then\n",
    "\n",
    "$$ \\Delta   (G) \\le  2 * \\Delta   (O) $$\n",
    "```\n",
    "\n",
    "```{prf:proof}    \n",
    "\n",
    "\n",
    "Take the points in the set H(k+1) = { h(1), h(2), .... , h(k+1)}    \n",
    "\n",
    "Since we have k circles and K+1 balls, there must be atleast two of these points in the same circle. <b>(Pigeonhole Principle) </b>\n",
    "\n",
    "say x and y are mapped to the same center c.   \n",
    "\n",
    "\n",
    "max(d(x, c), d(y, c)) $ \\le   \\Delta   (O) $   \n",
    "\n",
    "Since x, y $\\in   $ G(k+1),    \n",
    "\n",
    "$$ d(x, y) \\ge \\Delta   (k) = \\Delta   (G) \\text{ ...(lemma 2)}    \n",
    "\n",
    "\n",
    "\n",
    "\\Delta   (G) \\le  d(x, y) \\le d(x, c) + d(y, c)   \n",
    "\n",
    "\n",
    "d(x, c) + d(y, c) \\le   \\Delta   (O) + \\Delta   (O)   \n",
    "\n",
    "\n",
    "\\text{from above two equations,}    \n",
    "\n",
    "\n",
    "\\Delta   (G) \\le  2 * \\Delta   (O)\n",
    "\n",
    " $$\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{important} Can We Do Better? \n",
    "* We can do no better than the 2-approximation.\n",
    "* For any small constant ùúñ > 0, we cannot get a 2 - ùúñ approximation, unless P = NP. \n",
    "* If the distance function does not obey triangle inequality, then we cannot build any algorithm for any arbitrary constant factor approximation, unless P = NP. For specific distance functions and datasets, we might get good heuristics.\n",
    "\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "## K-Means  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0357aa0d4070941cff46fe927e94d9975f87421b9624348f0a16d87c232906dd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cs328': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
