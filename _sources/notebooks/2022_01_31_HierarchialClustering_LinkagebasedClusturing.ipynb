{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ?\n",
    "\n",
    "* Hierarchical Clustering can be used when we do not know the target number of clusters.\n",
    "\n",
    "* This clustering mechanism is also very useful because we can produce a family of clusters, for each k.\n",
    "\n",
    "``` {note} \n",
    "A lot of papers have been written on hierarchial clustering. \n",
    "Researchers use this hierarchy to match with the natural hierarchy to obtain results. \n",
    "Eg :- Hierarchical clustering is used in Gene Sequencing and DNA matching etc.\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways of doing Hierarchical Clustering:\n",
    "* Top-Down/divisive: bi-partition\n",
    "* Bottom up/agglomerative: aggregating from individual point\n",
    "\n",
    "``` {note} \n",
    "Single Linkage Clustering is one of the most common heirarchical clustering. It is an example of agglomerative clustering\n",
    "\n",
    "```{figure} ../assets/2022_01_31_HierarchialClustering_LinkagebasedClusturing/hierarch_1.gif\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three different ways of finding closeness between the clusters :-\n",
    "\n",
    "* Closest Pair: Single Linkage Clustering - The distance between any two pairs of clusters is given by the minimum distance between the elements of the pair of the cluster.\n",
    "\n",
    "$$\n",
    "d_1(C_i,C_j) = \\min _{x \\in C_i , y \\in C_j}d(x,y)\n",
    "$$\n",
    "\n",
    "* Farthest Pair: Complete Link Clustering - The distance between any two pairs of clusters is given by the maximum distance between the elements of the pair of the cluster.\n",
    "\n",
    "$$\n",
    "d_2(C_i,C_j) = \\max _{x \\in C_i , y \\in C_j}d(x,y)\n",
    "$$\n",
    "\n",
    "* Average of all Pairs: Average Link Clustering - The distance between any two pairs of clusters is given by the average distance between the elements of the pair of the cluster.\n",
    "\n",
    "$$\n",
    "d_2(C_i,C_j) = avg _{x \\in C_i , y \\in C_j}d(x,y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties Of Single Linkage Algorithm\n",
    "\n",
    "* The algorithm can work with any dissimilarity measure, it may not necessarily be metric\n",
    "\n",
    "* There are no Inversions.\n",
    "    * As we run the algorithm (as we start merging the clusters), the dissimilarity scores between merged clusters only increases.\n",
    "\n",
    "```{figure} ../assets/2022_01_31_HierarchialClustering_LinkagebasedClusturing/LevelComp.jpg\n",
    "```\n",
    "$$ Dissimilarity ~~ at ~~ Level - 2 > Dissimilarity ~~at ~~Level - 5 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function\n",
    "\n",
    "The question that arises while understanding the single linkage algorithm is in terms of it's objective function.\n",
    "\n",
    "   OR\n",
    "\n",
    "What kind of clusters is this algorithm trying to create?\n",
    "\n",
    "To understand this we would need to look up some properties of Single Linkage Clustering\n",
    "\n",
    "Let's suppose that we want to create a cluster $ C_1,C_2,....C_k $ which maximizes the inter cluster distance.\n",
    "\n",
    "$$\n",
    " maximize~~cost_k(C)\n",
    "$$\n",
    "$$\n",
    "cost_k(C)=\\min _{C_i,C_j}d(C_i,C_j) \n",
    "$$\n",
    "\n",
    "\n",
    "```{figure} ../assets/2022_01_31_HierarchialClustering_LinkagebasedClusturing/k_1.jpg\n",
    "```\n",
    "\n",
    "\n",
    "If the above task is our objective function (to maximize the minimum inter cluster distancefor k clusters) then we can easily use single linkage clustering to achieve our objective.\n",
    "\n",
    "We just have to remove the top k-1 clustering merges and we will achieve k clusters. Also, all these K clusters will follow the above objective function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Properties of Single Linkage Clustering\n",
    "\n",
    "* It is actually an Euclidean Minimum spanning Tree!\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../assets/2022_01_31_HierarchialClustering_LinkagebasedClusturing/c.jpg\n",
    "```\n",
    "  $$ Unlike ~~ centre-based ~~ clustering ~~ Single linkage ~~ clustering ~~ can ~ handle ~~ the ~~ above ~~ example ~~ nicely!.$$\n",
    "\n",
    "<footer>\n",
    "Author(s): Abhigyan M. Ninama, Chetan Kishore, Shubh Lavti\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca1377445b858ef3210e48761d2092d9e3e8f3658127ad2bce06f6112bc41a6b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
